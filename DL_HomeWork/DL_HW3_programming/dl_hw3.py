# -*- coding: utf-8 -*-
"""Copy of cs482_hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSgwiLWA1WDRiCMf7Hvm079SdJKAts61
"""

import math
import os
import datetime
import csv
# %matplotlib inline
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets
from torchvision import transforms
from torch.autograd import Variable
import numpy as np
import tqdm

from IPython import display


# The Args object will contain all of our parameters
# If you want to run with different arguments, create another Args object

class Args(object):
    def __init__(self, name='mnist', batch_size=64, test_batch_size=1000,
                 epochs=10, lr=0.01, optimizer='sgd', momentum=0.5,
                 seed=1, log_interval=100, dataset='mnist',
                 data_dir='data', model='default',
                 cuda=True):
        self.name = name  # name for this training run. Don't use spaces.
        self.batch_size = batch_size
        self.test_batch_size = test_batch_size  # Input batch size for testing
        self.epochs = epochs  # Number of epochs to train
        self.lr = lr  # Learning rate
        self.optimizer = optimizer  # sgd/p1sgd/adam/rms_prop
        self.momentum = momentum  # SGD Momentum
        self.seed = seed  # Random seed
        self.log_interval = log_interval  # Batches to wait before logging
        # detailed status. 0 = never
        self.dataset = dataset  # mnist/fashion_mnist
        self.data_dir = data_dir
        self.model = model  # default/P2Q7DoubleChannelsNet/P2Q7HalfChannelsNet/
        # P2Q8BatchNormNet/P2Q9DropoutNet/P2Q10DropoutBatchnormNet/
        # P2Q11ExtraConvNet/P2Q12RemoveLayerNet/P2Q13UltimateNet
        self.cuda = cuda and torch.cuda.is_available()


"""## New for homework 3:

* Implement NN classes below this
"""


# Utilities for classes
def _assert_no_grad(variable):
    assert not variable.requires_grad, ("nn criterions don't compute the gradient w.r.t. targets - please "
                                        "mark these variables as not requiring gradients")


class _Loss(nn.Module):
    def __init__(self, size_average=True):
        super(_Loss, self).__init__()
        self.size_average = size_average


class _WeightedLoss(_Loss):
    def __init__(self, weight=None, size_average=True):
        super(_WeightedLoss, self).__init__(size_average)
        self.register_buffer('weight', weight)


class P3SGD(optim.Optimizer):
    r"""Implements stochastic gradient descent (optionally with momentum).

    Args:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float): learning rate
        momentum (float, optional): momentum factor (default: 0)

    Example:
        >>> optimizer = P3SGD(model.parameters(), lr=0.1, momentum=0.9)
        >>> optimizer.zero_grad()
        >>> loss_fn(model(input), target).backward()
        >>> optimizer.step()

    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf

    .. note::
        The implementation of P3SGD with Momentum/Nesterov subtly differs from
        Sutskever et. al. and implementations in some other frameworks.
        Let p, g, v, :math:`\rho`, and d denote the parameters, gradient,
        velocity, momentum, and dampening respectively.

        Considering the specific case of Momentum, the update can be written as

        .. math::
                  v = \rho * v + g \\
                  p = p - lr * v

        This is in contrast to Sutskever et. al. and
        other frameworks which employ an update of the form

        .. math::
             v = \rho * v + lr * g \\
             p = p - v


    """

    def __init__(self, params, lr, momentum=0, dampening=0):
        defaults = dict(lr=lr, momentum=momentum)
        super(P3SGD, self).__init__(params, defaults)
        self.lr = lr
        self.momentum = momentum

    def __setstate__(self, state):
        super(P3SGD, self).__setstate__(state)
        for group in self.param_groups:
            group.setdefault('nesterov', False)

    def step(self, closure=None):
        """Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            momentum = self.momentum

            for p in group['params']:
                # TODO Implement me
                # raise NotImplementedError
                # You will also need to use:
                #
                # 1. The gradient of the current param
                #       d_p = p.grad.data
                # 2. The momentum buffer storing the previous step's momentum
                #       param_state['momentum_buffer']
                # 3. The current stored learning rate
                #       group['lr']

                # sanity check
                if p.grad is None:
                    continue

                g = p.grad.data

                if momentum != 0:

                    param_state = self.state[p]
                    if "momentum_buffer" not in param_state:
                        param_state['momentum_buffer'] = torch.zeros_like(p.data)
                        param_state['momentum_buffer'] = param_state['momentum_buffer'] * momentum + g
                        temp = param_state['momentum_buffer']
                        # p -= self.lr * param_state['momentum_buffer']
                        p.data = p.data - self.lr * temp
                    else:
                        param_state['momentum_buffer'] = param_state['momentum_buffer'] * momentum + g
                        temp = param_state['momentum_buffer']
                        p.data = p.data - self.lr * temp
                else:
                    raise ValueError("Momentum cannot be 0.")
        return loss


class P3Dropout(nn.Module):
    r"""During training, randomly zeroes some of the elements of the input
    tensor with probability *p* using samples from a bernoulli distribution.
    The elements to zero are randomized on every forward call.
    This has proven to be an effective technique for regularization and
    preventing the co-adaptation of neurons as described in the paper
    `Improving neural networks by preventing co-adaptation of feature
    detectors`_ .
    Furthermore, the outputs are scaled by a factor of :math:`\frac{1}{1-p}` during
    training. This means that during evaluation the module simply computes an
    identity function.
    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``
    Shape:
        - Input: `Any`. Input can be of any shape
        - Output: `Same`. Output is of the same shape as input
    Examples::
        >>> m = nn.Dropout(p=0.2)
        >>> input = torch.randn(20, 16)
        >>> output = m(input)
    .. _Improving neural networks by preventing co-adaptation of feature
        detectors: https://arxiv.org/abs/1207.0580
    """

    def __init__(self, p=0.5, inplace=False):
        super(P3Dropout, self).__init__()
        # TODO Implement me
        self.p = p
        self.inplace = inplace
        # raise NotImplementedError

    def forward(self, input):
        # TODO Implement me
        if self.training:
            keep_p = 1 - self.p
            scale_mask = keep_p + torch.FloatTensor(input.shape).uniform_(0.1)
            scale_mask = scale_mask.floor()

            # rescale
            scale_mask /= keep_p
            return input * scale_mask

        # otherwise, identical function
        return input

        # raise NotImplementedError

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return (self.__class__.__name__ + '(' + 'p=' + str(self.p) +
                inplace_str + ')')


class P3Dropout2d(nn.Module):
    r"""Randomly zeroes whole channels of the input tensor.
    The channels to zero-out are randomized on every forward call.
    Usually the input comes from :class:`nn.Conv2d` modules.
    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.
    In this case, :func:`nn.Dropout2d` will help promote independence between
    feature maps and should be used instead.
    Args:
        p (float, optional): probability of an element to be zero-ed.
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place
    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)
    Examples::
        >>> m = nn.Dropout2d(p=0.2)
        >>> input = torch.randn(20, 16, 32, 32)
        >>> output = m(input)
    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """

    def __init__(self, p=0.5, inplace=False):
        super(P3Dropout2d, self).__init__()
        # TODO Implement me
        self.p = p
        self.inpalce = inplace

    def forward(self, input):
        if self.training:
            keep_p = 1 - self.p
            # keep_p + uniform_dist(0~1) with shape [batch_size, num_channels]
            scale_mask = keep_p + torch.FloatTensor(input.shape[0], input.shape[3]).uniform_(0, 1)
            # create binary mask
            scale_mask = scale_mask.floor()
            # encode scale
            scale_mask /= keep_p
            # reshape to include h, w(1, 1) for broadcasting
            scale_mask = torch.reshape(scale_mask, (input.shape[0], 1, 1, input.shape[3]))
            # multiple - broadcasting along h, w
            return input * scale_mask
        else:
            # test time: identical mapping.
            return input

        # raise NotImplementedError

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' + 'p=' + str(self.p) + inplace_str + ')'


# def linear(input, weight, bias=None):
#     """
#     Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
#     Shape:
#         - Input: :math:`(N, *, in\_features)` where `*` means any number of
#           additional dimensions
#         - Weight: :math:`(out\_features, in\_features)`
#         - Bias: :math:`(out\_features)`
#         - Output: :math:`(N, *, out\_features)`
#     """
#     temp = input @ torch.t(weight)
#     if bias is not None:
#         temp += bias
#
#     return temp


def linear(input, weight, bias=None):
    # First braces create a Function object. Any arguments given here
    # will be passed to __init__. Second braces will invoke the __call__
    # operator, that will then use forward() to compute the result and
    # return it.
    return P3LinearFunction()(input, weight, bias)

    # raise NotImplementedError


# class P3LinearFunction(torch.autograd.Function):
#     """See P3Linear for details.
#     """
#     def __init__(self):
#         super(P3LinearFunction).__init__()
#
#
#     def forward(self, input, weight, bias=None):
#         # TODO Implement me
#         ### not so correct???
#         # raise NotImplementedError
#         # this is redundant
#         # self.weight = Variable(torch.randn(input.shape[1], input.shape[0]))
#         # self.weight = torch.randn(input.shape[1], input.shape[0], requires_grad=True)
#         # self.bias = torch.randn(1, requires_grad=True)
#         # self.input = input
#         # return linear(input, self.weight, self.bias)
#
#
#         self.save_for_backward(input, weight, bias)
#
#         output = input.mm(self.weight.t())
#         output += self.bias.unsqueeze(0).expand_as(output)
#         return output
#
#     def backward(self, grad_output):
#         """
#         In the backward pass we receive a Tensor containing the gradient of the loss
#         with respect to the output, and we need to compute the gradient of the loss
#         with respect to the input.
#         """
#         # TODO manually implement a backwards pass
#
#         input, weight, bias = self.saved_variables
#         grad_input = grad_weight = grad_bias = None
#
#         # if self.needs_input_grad[0]:
#         grad_input = grad_output.mm(weight)
#
#         # if self.needs_input_grad[1]:
#         grad_weight = grad_output.t().mm(input)
#
#         # if bias is not None and self.needs_input_grad[2]:
#         grad_bias = grad_output.sum(0).squeeze(0)
#
#         # input [N, 3] weight[6, 3] out [N, 6]
#         # grad_out [N, 6],   weight [6, 3] - > grad_out @ weight - > [N, 3] (shape as input)
#
#         return grad_input, grad_weight, grad_bias



# Inherit from Function
class P3LinearFunction(torch.autograd.Function):

    # Note that both forward and backward are @staticmethods
    def __init__(self):
        super(P3LinearFunction).__init__()

    # bias is an optional argument
    def forward(self, input, weight, bias=None):
        self.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    # This function has only a single output, so it gets only one gradient
    def backward(self, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = self.saved_variables
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if self.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if self.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and self.needs_input_grad[2]:
            grad_bias = grad_output.sum(0).squeeze(0)

        return grad_input, grad_weight, grad_bias


class P3Linear(nn.Module):
    r"""Applies a linear transformation to the incoming data: :math:`y = Ax + b`
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        bias: If set to False, the layer will not learn an additive bias.
            Default: ``True``
    Shape:
        - Input: :math:`(N, *, in\_features)` where :math:`*` means any number of
          additional dimensions
        - Output: :math:`(N, *, out\_features)` where all but the last dimension
          are the same shape as the input.
    Attributes:
        weight: the learnable weights of the module of shape
            `(out_features x in_features)`
        bias:   the learnable bias of the module of shape `(out_features)`
    Examples::
        >>> m = nn.Linear(20, 30)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
    """

    def __init__(self, in_features, out_features, bias=True):
        super(P3Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = torch.nn.Parameter(torch.Tensor(self.out_features, self.in_features))

        if bias:
            self.bias = torch.nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()


    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        # TODO Implement me
        temp = input @ torch.t(self.weight)
        if self.bias is not None:
            temp += self.bias
        return temp

    def __repr__(self):
        return (self.__class__.__name__ + '(' +
                'in_features=' + str(self.in_features) +
                ', out_features=' + str(self.out_features) +
                ', bias=' + str(self.bias is not None) + ')')


def p3relu(input, inplace=False):
    r"""relu(input, inplace=False) -> Tensor
    Applies the rectified linear unit function element-wise.
    :math:`\text{ReLU}(x)= \max(0, x)`
    .. image:: _static/img/activation/ReLU.png
    Args:
        inplace: can optionally do the operation in-place. Default: ``False``
    """
    # TODO Implement me
    return torch.clamp(input, min=0)


class P3ReLU(nn.Module):
    r"""Applies the rectified linear unit function element-wise
    :math:`\text{ReLU}(x)= \max(0, x)`
    .. image:: _static/img/activation/ReLU.png
    Args:
        inplace: can optionally do the operation in-place. Default: ``False``
    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input
    Examples::
        >>> m = nn.ReLU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, inplace=False):
        super(P3ReLU, self).__init__()

    def forward(self, input):
        # TODO Implement me
        return torch.clamp(input, min=0)

    def __repr__(self):
        inplace_str = 'inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
               + inplace_str + ')'


class P3ReLU(nn.Module):
    r"""Applies the rectified linear unit function element-wise
    :math:`\text{ReLU}(x)= \max(0, x)`
    .. image:: _static/img/activation/ReLU.png
    Args:
        inplace: can optionally do the operation in-place. Default: ``False``
    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input
    Examples::
        >>> m = nn.ReLU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, inplace=False):
        super(P3ReLU, self).__init__()

    def forward(self, input):
        # TODO Implement me
        return torch.clamp(input, min=0)

    def __repr__(self):
        inplace_str = 'inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
               + inplace_str + ')'


class P3ELUFunction(torch.autograd.Function):
    r"""Applies element-wise,
    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`
    Args:
        alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0
        inplace: can optionally do the operation in-place. Default: ``False``
    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input
    .. image:: _static/img/activation/ELU.png
    Examples::
        >>> m = nn.ELU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def __init__(self):
        super(P3ELUFunction, self).__init__()


    def forward(self, input, alpha=1.0):
        mask1 = (input >= 0).float()
        mask2 = (input < 0).float()
        alpha = torch.FloatTensor([alpha])
        combined = mask1 * input + mask2 * (torch.exp(input) - 1)
        self.save_for_backward(mask1, mask2, input, alpha)
        return combined

    def backward(self, grad_output):
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        # TODO manually implement a backwards pass
        mask1, mask2, input, alpha = self.saved_tensors
        combined_grad = (mask1 + mask2 * 1 * torch.exp(input)) * grad_output
        return combined_grad


class P3ELU(nn.Module):
    r"""Applies element-wise,
    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`
    Args:
        alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0
        inplace: can optionally do the operation in-place. Default: ``False``
    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input
    .. image:: _static/img/activation/ELU.png
    Examples::
        >>> m = nn.ELU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, alpha=1., inplace=False):
        super(P3ELU, self).__init__()
        self.alpha = alpha
        self.inplace = inplace
        self.alpha = alpha


    def forward(self, input):
        self.mask1 = (input >= 0).float()
        self.mask2 = (input < 0).float()
        combined = self.mask1 * input + self.mask2 * (torch.exp(input) - 1)
        return combined

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return (self.__class__.__name__ + '(' +
                'alpha=' + str(self.alpha) + inplace_str + ')')


class P3BCELoss(_WeightedLoss):
    r"""Creates a criterion that measures the Binary Cross Entropy
    between the target and the output:
    The loss can be described as:
    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],
    where :math:`N` is the batch size. If reduce is ``True``, then
    .. math::
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if}\; \text{size_average} = \text{True},\\
            \operatorname{sum}(L),  & \text{if}\; \text{size_average} = \text{False}.
        \end{cases}
    This is used for measuring the error of a reconstruction in for example
    an auto-encoder. Note that the targets `y` should be numbers
    between 0 and 1.
    Args:
        weight (Tensor, optional): a manual rescaling weight given to the loss
            of each batch element. If given, has to be a Tensor of size
            "nbatch".
        size_average (bool, optional): By default, the losses are averaged
            over observations for each minibatch. However, if the field
            size_average is set to ``False``, the losses are instead summed for
            each minibatch. Default: ``True``
        reduce (bool, optional): By default, the losses are averaged or summed over
            observations for each minibatch depending on size_average. When reduce
            is False, returns a loss per input/target element instead and ignores
            size_average. Default: True
    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar. If `reduce` is False, then `(N, *)`, same shape as
          input.
    Examples::
        >>> m = nn.Sigmoid()
        >>> loss = nn.BCELoss()
        >>> input = torch.randn(3, requires_grad=True)
        >>> target = torch.FloatTensor(3).random_(2)
        >>> output = loss(m(input), target)
        >>> output.backward()
    """


    def forward(self, input, target):
        _assert_no_grad(target)
        # TODO implement me
        # sanity check

        num_classes = input.size(1)
        assert input.size(0) == target.size(0)
        # one hot label for target size for processing

        # convert input to prob

        # convert to numpy
        # input = input.cpu().data.numpy()
        # tt = np.amax(input, axis=1)
        # tt = tt.reshape(len(tt), 1)
        # input -= tt
        # prob = (np.exp(input).T / np.sum(np.exp(input), axis=1)).T
        #
        #
        # # (-1/m) * sum_(y_hat(i) * pred_prob(i))
        # nll_loss = (-1 / input.shape[0]) * np.sum(temp * np.log(prob))
        # nll_loss = np.array(nll_loss, dtype=np.float32)
        # nll_loss = torch.autograd.Variable(torch.Tensor(nll_loss))

        # one hot embedding using basis trick..
        # this will be: shape (64, 10) with one-hot encoded
        logsoftmax = nn.LogSoftmax()
        basis = torch.eye(input.size(1))
        target_one_hot = basis[target]

        # -1/m *
        return torch.mean(torch.sum(-target_one_hot * logsoftmax(input), dim=1))


# Define the neural network classes
# (Change this to use your new modules)

# class Net(nn.Module):
#     def __init__(self):
#         super(Net, self).__init__()
#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
#         self.fc1 = nn.Linear(320, 50)
#         self.fc2 = nn.Linear(50, 10)
#
#     def forward(self, x):
#         # F is just a functional wrapper for modules from the nn package
#         # see http://pytorch.org/docs/_modules/torch/nn/functional.html
#         x = F.relu(F.max_pool2d(self.conv1(x), 2))
#         x = F.relu(F.max_pool2d(self.conv2(x), 2))
#         x = x.view(-1, 320)
#         x = F.relu(self.fc1(x))
#         x = F.dropout(x, training=self.training)
#         x = self.fc2(x)
#         return F.log_softmax(x, dim=1)



# "Stateful" version (i.e. make use of nn.Module implementation)
# class Net(nn.Module):
#     def __init__(self):
#         super(Net, self).__init__()
#         self.fc0 = P3Linear(28*28, 320)
#         self.fc1 = P3Linear(320, 50)
#         self.fc2 = P3Linear(50, 10)
#         self.elu1 = P3ELU()
#         self.elu2 = P3ELU()
#         self.dropout = P3Dropout2d()
#
#
#     def forward(self, x):
#         # F is just a functional wrapper for modules from the nn package
#         # see http://pytorch.org/docs/_modules/torch/nn/functional.html
#
#         x = x.view(-1, 28*28)
#         x = self.elu1(self.fc0(x))
#         x = self.elu2(self.fc1(x))
#         x = self.dropout(x)
#         x = self.fc2(x)
#         return F.log_softmax(x, dim=1)


# nn.Module versions (stateful)
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)

        # Linear -- extend nn.Module version
        self.fc1 = P3Linear(320, 50)
        # self.fc2 = P3Linear(50, 10)

        # ReLU + ELU - extend nn.Module version
        self.elu1 = P3ELU()
        self.elu2 = P3ELU()
        self.relu = P3ReLU()

        # dropout and dropout 2d - extend nn.Module version
        self.dropout = P3Dropout()
        self.dropout2d = P3Dropout2d()

        # this is for final layer -- P3LinearFunction(autograd.Function)
        self.weight_final_layer = Variable(torch.randn(10, 50))
        self.bias_final_layer = Variable(torch.randn(10))

    def forward(self, x):

        # P3ELU(nn.Module)
        x = self.elu1((F.max_pool2d(self.conv1(x), 2)))
        # P3Dropout2d(nn.Module)
        x = self.dropout2d(x)
        # P3ELUFunction(autograd.Function)
        x = P3ELUFunction()(F.max_pool2d(self.conv2(x), 2))
        x = x.view(-1, 320)
        # P3ReLU(nn.Module) + P3Linear(nn.Module)
        x = self.relu(self.fc1(x))
        # P3Dropout(nn.Module)
        x = self.dropout(x)
        # P3LinearFunction(autograd.Function)
        x = P3LinearFunction()(x, self.weight_final_layer, self.bias_final_layer)
        return F.log_softmax(x, dim=1)


"""## The rest should be familiar"""


def prepare_dataset(args):
    # choose the dataset
    if args.dataset == 'mnist':
        DatasetClass = datasets.MNIST
    elif args.dataset == 'fashion_mnist':
        DatasetClass = datasets.FashionMNIST
    else:
        raise ValueError('unknown dataset: ' + args.dataset +
                         ' try mnist or fashion_mnist')

    def time_stamp(fname, fmt='%m-%d-%H-%M_{fname}'):
        return datetime.datetime.now().strftime(fmt).format(fname=fname)

    training_run_name = time_stamp(args.dataset + '_' + args.name)

    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}

    # Create the dataset: mnist or fasion_mnist
    dataset_dir = os.path.join(args.data_dir, args.dataset)
    training_run_dir = os.path.join(args.data_dir, training_run_name)
    train_dataset = DatasetClass(
        dataset_dir, train=True, download=True,
        transform=transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ]))
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)
    test_dataset = DatasetClass(
        dataset_dir, train=False, transform=transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ]))
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=args.test_batch_size, shuffle=True, **kwargs)

    if not os.path.exists(training_run_dir):
        os.makedirs(training_run_dir)

    return train_loader, test_loader, train_dataset, test_dataset, training_run_dir


# visualize some images

args = Args()
_, _, _, test_dataset, _ = prepare_dataset(args)
images = test_dataset.test_data[:6]
labels = test_dataset.test_labels[:6]
fig, axes = plt.subplots(1, 6)
for axis, img, lbl in zip(axes, images, labels):
    axis.imshow(img)
    axis.set_title(lbl)
    axis.set_yticklabels([])
    axis.set_xticklabels([])
plt.show()


def train(model, optimizer, train_loader, epoch, total_minibatch_count,
          train_losses, train_accs):
    # Training for a full epoch

    model.train()
    correct_count, total_loss, total_acc = 0., 0., 0.
    progress_bar = tqdm.tqdm(train_loader, desc='Training')

    for batch_idx, (data, target) in enumerate(progress_bar):
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)

        optimizer.zero_grad()

        # Forward prediction step
        output = model(data)
        # loss = F.nll_loss(output, target)

        # use loss
        BCELoss = P3BCELoss()
        loss = BCELoss(output, target)
        # Backpropagation step
        loss.backward()
        optimizer.step()

        # The batch has ended, determine the accuracy of the predicted outputs
        pred = output.data.max(1)[1]

        # target labels and predictions are categorical values from 0 to 9.
        matches = target == pred
        accuracy = matches.float().mean()
        correct_count += matches.sum()

        if args.log_interval != 0 and \
                total_minibatch_count % args.log_interval == 0:
            train_losses.append(loss.data[0])
            train_accs.append(accuracy.data[0])

        total_loss += loss.data
        total_acc += accuracy.data

        progress_bar.set_description(
            'Epoch: {} loss: {:.4f}, acc: {:.2f}'.format(
                epoch, total_loss / (batch_idx + 1), total_acc / (batch_idx + 1)))
        # progress_bar.refresh()

        total_minibatch_count += 1

    return total_minibatch_count


def test(model, test_loader, epoch, total_minibatch_count,
         val_losses, val_accs):
    # Validation Testing
    model.eval()
    test_loss, correct = 0., 0.
    progress_bar = tqdm.tqdm(test_loader, desc='Validation')
    with torch.no_grad():
        for data, target in progress_bar:
            if args.cuda:
                data, target = data.cuda(), target.cuda()
            data, target = Variable(data), Variable(target)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').data  # sum up batch loss
            pred = output.data.max(1)[1]  # get the index of the max log-probability
            correct += (target == pred).float().sum()

    test_loss /= len(test_loader.dataset)

    acc = correct / len(test_loader.dataset)

    val_losses.append(test_loss)
    val_accs.append(acc)

    progress_bar.clear()
    progress_bar.write(
        '\nEpoch: {} validation test results - Average val_loss: {:.4f}, val_acc: {}/{} ({:.2f}%)'.format(
            epoch, test_loss, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))

    return acc


# Run the experiment
def run_experiment(args):
    total_minibatch_count = 0

    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

    train_loader, test_loader, _, _, run_path = prepare_dataset(args)

    epochs_to_run = args.epochs

    # Choose model
    # TODO add all the other models here if their parameter is specified
    if args.model == 'default' or args.model == 'P2Q7DefaultChannelsNet':
        model = Net()
    elif args.model in globals():
        model = globals()[args.model]()
    else:
        raise ValueError('Unknown model type: ' + args.model)

    if args.cuda:
        model.cuda()

    # Choose optimizer
    if args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

    elif args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters())
    elif args.optimizer == 'rmsprop':
        optimizer = optim.RMSprop(model.parameters())
    elif args.optimizer == 'P3SGD':
        optimizer = P3SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

    else:
        raise ValueError('Unsupported optimizer: ' + args.optimizer)

    # Run the primary training loop, starting with validation accuracy of 0
    val_acc = 0
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []

    for epoch in range(1, epochs_to_run + 1):
        # train for 1 epoch
        total_minibatch_count = train(model, optimizer, train_loader,
                                      epoch, total_minibatch_count,
                                      train_losses, train_accs)
        # validate progress on test dataset
        val_acc = test(model, test_loader, epoch, total_minibatch_count,
                       val_losses, val_accs)

    fig, axes = plt.subplots(1, 4, figsize=(13, 4))
    # plot the losses and acc
    plt.title(args.name)
    axes[0].plot(train_losses)
    axes[0].set_title("Loss")
    axes[1].plot(train_accs)
    axes[1].set_title("Acc")
    axes[2].plot(val_losses)
    axes[2].set_title("Val loss")
    axes[3].plot(val_accs)
    axes[3].set_title("Val Acc")

    # Write to csv file
    with open(os.path.join(run_path + 'train.csv'), 'w') as f:
        csvw = csv.writer(f, delimiter=',')
        for loss, acc in zip(train_losses, train_accs):
            csvw.writerow((loss, acc))

    # Predict and Test
    images, labels = next(iter(test_loader))
    if args.cuda:
        images, labels = images.cuda(), labels.cuda()
    output = model(images)
    predicted = torch.max(output, 1)[1]
    fig, axes = plt.subplots(1, 6)
    for i, (axis, img, lbl) in enumerate(zip(axes, images, predicted)):
        if i > 5:
            break
        img = img.permute(1, 2, 0).squeeze()
        axis.imshow(img)
        axis.set_title(lbl.data)
        axis.set_yticklabels([])
        axis.set_xticklabels([])

    if args.dataset == 'fashion_mnist' and val_acc > 0.92 and val_acc <= 1.0:
        print("Congratulations, you beat the Question 13 minimum of 92"
              "with ({:.2f}%) validation accuracy!".format(val_acc))

run_experiment(Args(dataset="fashion_mnist", optimizer='P3SGD'))


